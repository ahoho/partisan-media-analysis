{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "layout: post\n",
    "title:  \"Alternate Realities: Article Cleaning and Initial Analysis\"\n",
    "date:   2016-10-23 20:01:00 -0500\n",
    "categories: partisan media analysis\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A first look at the articles using n-grams\n",
    "In my previous post, we collected roughly 500,000 articles from 80 left- and right-aligned online news sources, going back to July 2015. Here, we'll start to dive into the data.\n",
    "\n",
    "The first step we'll take is to going to identify the most common n-grams for each source. n-grams are ordered sets of words of length n, generated from a larger ordered set. For example, let's take a look at an iconic phrase from the campaign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Nobody',), ('has',), ('more',), ('respect',), ('for',), ('women',), ('than',), ('me',)]\n",
      "[('Nobody', 'has'), ('has', 'more'), ('more', 'respect'), ('respect', 'for'), ('for', 'women'), ('women', 'than'), ('than', 'me')]\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Nobody has more respect for women than me\"\n",
    "def gen_n_gram(sentence, n):\n",
    "    words = sentence.split()\n",
    "    return [tuple(words[i:i+n]) for i in range(len(words) - n + 1)]\n",
    "\n",
    "print(gen_n_gram(sentence, 1))\n",
    "print(gen_n_gram(sentence, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll rely on [scikit-learn's]() built-in functionality to generate n-grams, process the text, and place the data in a [term-document-matrix](). In order to do this, we can make an generator to return all articles from a query -- we'll be using similar functionality in the future, so I've opted to make it a class in order to allow us to inherit from it in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#stream.py\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "class QueryStream(object):\n",
    "    \"\"\" \n",
    "    Stream documents from the articles database\n",
    "    Can be subclassed to stream words or sentences from each document\n",
    "    \"\"\"\n",
    "    def __init__(self, sqldb, query=None, textcol='article_text', chunksize=1000, **kwargs):\n",
    "\n",
    "        self.sql_engine = create_engine(sqldb)\n",
    "        self.query = query\n",
    "        self.chunksize = chunksize\n",
    "        self.analyze = CountVectorizer(**kwargs).build_analyzer()\n",
    "        self.textcol = textcol\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\" Iterate through each row in the query \"\"\"\n",
    "        query_results = self.sql_engine.execute(self.query)\n",
    "        result_set = query_results.fetchmany(self.chunksize)\n",
    "        while result_set:\n",
    "            for row in result_set:\n",
    "                yield getattr(row, self.textcol)\n",
    "            result_set = query_results.fetchmany(self.chunksize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sql_url = 'postgres://postgres:postgres@localhost/articles'\n",
    "query = \"\"\"\n",
    "        SELECT post_id, article_text FROM articles\n",
    "        WHERE num_words > 200\n",
    "        ORDER BY post_id\n",
    "        \"\"\"\n",
    "query = \"\"\"\n",
    "        SELECT id, text as article_text FROM articles_old\n",
    "        WHERE length(text) > 1000\n",
    "        ORDER BY id\n",
    "        \"\"\"\n",
    "\n",
    "id_stream = QueryStream(sql_url, query, textcol='post_id')\n",
    "post_ids = [id for id in id_stream]\n",
    "\n",
    "text_stream = QueryStream(sql_url, query)\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 6), min_df=0.10)\n",
    "tdm = vectorizer.fit_transform(text_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get set of all page urls\n",
    "sql_url = 'postgres://postgres:postgres@localhost/articles'\n",
    "url_query = \"\"\"\n",
    "            SELECT base_url, count(*) as num_posts\n",
    "            FROM articles\n",
    "            GROUP BY base_url\n",
    "            \"\"\"\n",
    "results = create_engine(sql_url).execute(url_query).fetchall()\n",
    "base_urls = [row.base_url for row in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_source_tdm(base_url, sqldb):\n",
    "    \"\"\"Create and save a document-term matrix for a given source\"\"\"\n",
    "    query = \"\"\"\n",
    "            SELECT article_text FROM articles\n",
    "            WHERE word_count > 200\n",
    "            \"\"\".format(base_url)\n",
    "    stream = QueryStream(sqldb, query)\n",
    "    \n",
    "    #only keep n-grams that appear in over 10% of articles\n",
    "    vectorizer = CountVectorizer(ngram_range=(1, 6), min_df=0.10)\n",
    "    tdm = vectorizer.fit_transform(stream)\n",
    "    \n",
    "    #save\n",
    "    \n",
    "    \n",
    "    return \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# word count:\n",
    "#array_length(regexp_split_to_array(article_text, E'\\\\W+'), 1) > 200\n",
    "query = \"\"\"\n",
    "        SELECT article_text FROM articles\n",
    "        WHERE base_url like '%%breitbart%%' and\n",
    "              array_length(regexp_split_to_array(trim(article_text), E'\\\\\\W+'), 1) > 200\n",
    "        LIMIT 5\n",
    "        \"\"\"\n",
    "stream = QueryStream(sqldb = 'postgres://postgres:postgres@localhost/articles',\n",
    "                     query=query)\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 6), min_df=0.10)\n",
    "tdm = vectorizer.fit_transform(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 12046)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'am': 0, 'crook': 1, 'friend': 2, 'is': 3, 'joe': 4, 'my': 5, 'not': 6}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(min_df=0.10)\n",
    "x = vectorizer.fit_transform(['my friend is a joe', 'my joe is a friend', 'i am not a crook'])\n",
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = vectorizer.fit_transform(['justin is a boy', 'justin is a girl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "inconsistent shapes",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-b50c5120e24b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Anaconda2\\envs\\py3k\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36m__add__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misspmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"inconsistent shapes\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_binopt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'_plus_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: inconsistent shapes"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<100x363874 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 413662 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer()\n",
    "transformer.fit_transform(X)\n",
    "\n",
    "#TODO:\n",
    "    #DO SIMPLE SUMMARY STATS ON RETRIEVED DOCS (word lengths, etc)\n",
    "    #TREAT EACH SOURCE AS A DOCUMENT\n",
    "    #RUN TF-IDF TO GET EACH SOURCE'S MOST UNIQUE N-GRAMS\n",
    "    #PLOT N-GRAMS on alignment axis, perhaps indicating which n-grams come from where\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3k]",
   "language": "python",
   "name": "conda-env-py3k-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "layout: post\n",
    "title:  \"Alternate Realities: Article Processing\"\n",
    "date:   2016-12-30 20:01:00 -0500\n",
    "categories: partisan media analysis\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Processing\n",
    "### [In progress]\n",
    "In a previous post, we collected roughly 500,000 articles from 80 left- and right-aligned online news sources, going back to July 2015. Here, we'll start to clean and process the text data to enable future analyses.\n",
    "\n",
    "This is sort of an iterative problem -- I'll likely be revising this process to account for noise that makes it through. This is another post heavy on mechanics and light on findings, but the next one will have some interesting results. \n",
    "\n",
    "First, we'll develop a means of accessing text from the database by building an extensible generator. Then, we'll stream sentences to find commonly occuring n-grams in the data, and finally we'll pick those n-grams that we want to treat as single words in the future. \n",
    "\n",
    "We'll rely on a combination of [nltk](http://www.nltk.org/), [gensim](https://radimrehurek.com/gensim/index.html), and [scikit-learn's](http://scikit-learn.org/stable/index.html) to tokenize our documents, generate n-grams, do some POS tagging, and place the data in a term-document-matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming tokenized text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we'll create a generator to return all articles from a query. We'll be using similar functionality throughout this process, so I've opted to make it a class so that we may inherit from it in the future and stream, for e.g., sentences for use in Word2Vec models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "class QueryStream(object):\n",
    "    \"\"\" \n",
    "    Stream documents from the articles database\n",
    "    Can be subclassed to stream words or sentences from each document\n",
    "    \"\"\"\n",
    "    def __init__(self, sqldb, query=None, idcol='post_id', \n",
    "                 textcol='article_text', chunksize=1000):\n",
    "\n",
    "        self.sql_engine = create_engine(sqldb)\n",
    "        self.query = query\n",
    "        self.chunksize = chunksize\n",
    "        self.textcol = textcol\n",
    "        self.idcol = idcol\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\" Iterate through each row in the query \"\"\"\n",
    "        query_results = self.sql_engine.execute(self.query)\n",
    "        result_set = query_results.fetchmany(self.chunksize)\n",
    "        while result_set:\n",
    "            for row in result_set:\n",
    "                yield row\n",
    "            result_set = query_results.fetchmany(self.chunksize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to create a pipeline for tokenizing documents that will split a document into sentences, then those sentences into words. Here, we'll use nltk for tokenization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize import MWETokenizer\n",
    "\n",
    "import regex as re\n",
    "\n",
    "class SentenceStream(QueryStream):\n",
    "    \"\"\"\n",
    "    Stream tokenized sentences from a query\n",
    "    \"\"\"\n",
    "    def __init__(self, ngrams=None, *args, **kwargs):\n",
    "        super(SentenceStream, self).__init__(*args, **kwargs)\n",
    "        # this will allow us to treat pre-defined n-grams as\n",
    "        # a single word, e.g., 'hillary_clinton', once\n",
    "        # we've identified them\n",
    "        self.mwe = MWETokenizer(ngrams)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        rows = super(SentenceStream, self).__iter__()\n",
    "        # remove all punctuation, except hyphens\n",
    "        punct = re.compile(\"[^A-Za-z0-9\\-]\")\n",
    "        \n",
    "        for doc in rows:\n",
    "            id = getattr(doc, self.idcol)\n",
    "            text = getattr(doc, self.textcol)\n",
    "            \n",
    "            for sentence in sent_tokenize(text):\n",
    "                split_sentence = [punct.sub('', word).lower()\n",
    "                                  for word in word_tokenize(sentence)]\n",
    "                yield id, self.mwe.tokenize([word for word in split_sentence \n",
    "                                             if word.replace('-', '')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying Collocations\n",
    "\n",
    "We'll use the article text we've scraped to identify n-grams like 'donald trump'. Should we deem it necessary, we can later identify synonyms like 'senator sanders' and 'bernie sanders' using Word2Vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from itertools import imap \n",
    "\n",
    "sql_url = 'postgres://postgres:**PASSWORD**@localhost/articles'\n",
    "\n",
    "full_query = \"\"\"\n",
    "             SELECT post_id, article_text\n",
    "             FROM articles\n",
    "             WHERE num_words > 100\n",
    "             \"\"\"\n",
    "\n",
    "# Since I'm running this on a Google Compute instance, I can afford\n",
    "# to load everything in memory as a list. While this isn't strictly necessary,\n",
    "# I can now avoid pulling from the database multiple times\n",
    "stream = list(SentenceStream(sqldb=sql_url, query=full_query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate, here's the 9th sentence in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'144317282271701_1045074145529339',\n",
       "  [u'a', u'mixture', u'of', u'motives', u'is', u'on', u'display'])]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream[9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must iteratively generate collocations from the sentence stream. The bigram object contains things like 'marco rubio', the trigram might now include 'senator marco rubio'. I imagine this can be improved down the line but for now it's sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "phrase_kwargs = {'threshold': 10,\n",
    "                  'min_count': 50}\n",
    "\n",
    "bigram = Phrases(imap(lambda x: x[1], stream), **phrase_kwargs)\n",
    "trigram = Phrases(bigram[imap(lambda x: x[1], stream)], **phrase_kwargs)\n",
    "quadgram = Phrases(trigram[imap(lambda x: x[1], stream)], **phrase_kwargs)\n",
    "\n",
    "phraser = Phraser(quadgram)\n",
    "phraser.save('../intermediate/phraser_all.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trimming n-grams\n",
    "Many of the n-grams we found have stopwords at either the end or beginning, for instance, 'the supreme court'. We'd like to trim these so that they are individually meaningful components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from pickle import cPickle\n",
    "\n",
    "def trim_phrases(phraser):\n",
    "    \"\"\"\n",
    "    Remove stopwords at the start and end of an ngram,\n",
    "    generate list of unique ngrams in corpus\n",
    "    \"\"\"\n",
    "    stop = stopwords.words('english')\n",
    "    ngrams = defaultdict(tuple)\n",
    "    for bigram, score in phraser.phrasegrams.items():\n",
    "        ngram = bigram[0].split('_') + bigram[1].split('_')\n",
    "        \n",
    "        idx = [i for i, v in enumerate(ngram) if v not in stop]  \n",
    "        ngram = ngram[idx[0]:idx[-1] + 1] if idx else []\n",
    "        \n",
    "        \n",
    "        if len(ngram) > 1:\n",
    "            ngrams[tuple(ngram)] = score\n",
    "                        \n",
    "    return ngrams\n",
    "  \n",
    "ngrams = trim_phrases(phraser)\n",
    "\n",
    "with open('../intermediate/phrasegrams_all.pkl', 'wb') as o:\n",
    "    pickle.save(ngrams, o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging\n",
    "Of the n-grams collected above, we're largely interested in noun-phrases like 'aborted_baby_parts' (appearing 1,062 times: uh, ok). rather than adjective or verb phrases like 'radical_islamic'. To subset to only nouns, we'll use [Part-of-Speech tagging](http://www.nltk.org/book/ch05.html) to see how these n-grams are employed in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "sql_url = 'postgres://postgres:**PASSWORD**@localhost/articles'\n",
    "full_query = \"\"\"\n",
    "             SELECT post_id, article_text\n",
    "             FROM articles\n",
    "             WHERE num_words > 100\n",
    "             \"\"\"\n",
    "with open('../intermediate/phrasegrams_all.pkl', 'rb') as i:\n",
    "    ngrams = pickle.load(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "class POSSentenceStream(SentenceStream):\n",
    "    \"\"\"\n",
    "    Assign parts of speech to n-grams\n",
    "    \"\"\"\n",
    "    def __iter__(self):\n",
    "        sentences = super(POSSentenceStream, self).__iter__()\n",
    "        for id, sentence in sentences:\n",
    "            for word, pos in pos_tag(sentence):\n",
    "                if '_' in word:\n",
    "                    yield word, pos\n",
    "            \n",
    "pos_words = POSSentenceStream(sqldb=sql_url, query=full_query, ngrams=ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To take a look at what I'm referring to, see some examples below. Just look: already we have the hugely loaded term *conversion_therapy*!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'hasnt_stopped', 'VBD'),\n",
       " (u'republican_politicians', 'NNS'),\n",
       " (u'radio_show', 'NN'),\n",
       " (u'rafael_cruz', 'VBD'),\n",
       " (u'ted_cruz', 'NN'),\n",
       " (u'barack_obama', 'NN'),\n",
       " (u'conversion_therapy', 'NN'),\n",
       " (u'reminds_us', 'NN'),\n",
       " (u'conversion_therapy', 'NN'),\n",
       " (u'sharp_contrast', 'NN')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might expect that certain n-grams, like 'presidential_candidate', could be considered both a noun or adjective phrase. As a result, we'll determine the POS distribution for each phrase, selecting only those that are nouns over 75% of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def count_ngram_occurances(pos_stream):\n",
    "    \"\"\"\n",
    "    Tally up the different POS associated with each n-gram\n",
    "    \"\"\"\n",
    "    pos_counts = {word: Counter() for word, pos in pos_stream}\n",
    "    \n",
    "    for word, pos in pos_stream:\n",
    "        pos_counts[word].update([pos])\n",
    "        \n",
    "    return pos_counts\n",
    "\n",
    "def identify_np_ngrams(pos_counts):\n",
    "    \"\"\"\n",
    "    Determine the n-grams that are most often employed as noun phrases\n",
    "    \"\"\"     \n",
    "    np_ngrams = defaultdict(str)\n",
    "    \n",
    "    for word, counts in pos_counts.items():\n",
    "        split_word = tuple(word.split('_'))\n",
    "                \n",
    "        word_count = sum(counts.values())        \n",
    "        noun_count = sum([v for k, v in counts.items() if 'NN' in k])\n",
    "        \n",
    "        # is it usually used as a noun?\n",
    "        if np.true_divide(noun_count, word_count) > 0.75:\n",
    "            np_ngrams[split_word] = word_count\n",
    "            \n",
    "    return np_ngrams\n",
    "            \n",
    "pos_counts = count_ngram_occurances(pos_words)  \n",
    "noun_ngrams = identify_np_ngrams(pos_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Already, we're seeing some moderately interesting findings in the data. For instance, *rigged election by hillary clinton* appears 113 times in 57 sources, *congress must investigate planned parenthood* 58 times over 16 sources. For posterity, I've saved these n-grams and their counts to github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save n-grams appearing over 25 times in the data\n",
    "ngram_data = pd.DataFrame.from_records(pos_counts)\\\n",
    "                         .transpose()\n",
    "    \n",
    "ngram_data['total'] = ngram_data.sum(axis=1)\n",
    "ngram_data.loc[ngram_data.total >= 25]\\\n",
    "          .to_csv('../output/ngram_pos_counts.csv', index_label='n-gram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../intermediate/noun_ngrams_all.pkl', 'wb') as o:\n",
    "    pickle.dump(noun_ngrams, o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that, we're done with this initial stage of processing. I imagine that I'll be expanding this post as time goes on to accomodate other issues that arise (already, I'm aware that there's a lot of noise in the data, to the tune of \"follow breitbart on Twitter\" or \"photo credit AP\", but I'd rather get some results first)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

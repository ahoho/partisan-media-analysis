{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "layout: post\n",
    "title:  \"Alternate Realities: A First Look at Partisan Word Use\"\n",
    "date:   2016-12-30 11:48:00 -0500\n",
    "categories: partisan media analysis\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partisan Word Use\n",
    "### [In progress]\n",
    "\n",
    "In the prior post, we created a means of reading tokenized data from our database of articles and a list of common noun phrases (like *supreme_court*) that we'll impose on our data to count certain n-grams as a single phrase.\n",
    "\n",
    "Now we're going to engage in a preliminary and fairly rudimentary analysis of the text data as it relates to partisanship. In order to do so, we'll look at which n-grams are most characteristic of the left and the right (later looking at sources individually). We'll expect results sort of in line with this [538 piece](http://fivethirtyeight.com/features/these-are-the-phrases-each-gop-candidate-repeats-most/) on the text of the GOP debates.\n",
    "\n",
    "This analysis will also set us up for topic modeling, out of fashion as it may be, since we'll just be building a large document-term matrix. I also hope to reapply it to the data on a weekly basis, to see what trends we might find in coverage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Document-Term-Matrix\n",
    "\n",
    "#### Reading the Data\n",
    "First, we want to collect the data at the level of each source, which will require us to subclass the `SentenceStream` generator we built in the last post. `CountVectorizer` from `scikit-learn` treats each element in an iterator as a document, so we'll restructure the generator such that all words from each source are combined into one list of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import groupby, imap\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "\n",
    "from stream import SentenceStream #from the prior post\n",
    "\n",
    "sql_url = 'postgres://postgres:Pluto2015!@localhost/articles'\n",
    "\n",
    "# ordering is a necessity, else groupby won't work\n",
    "query = \"\"\"\n",
    "        SELECT base_url, article_text\n",
    "        FROM articles\n",
    "        WHERE num_words > 100\n",
    "        ORDER BY base_url\n",
    "        \"\"\"\n",
    "\n",
    "class SourceStream(SentenceStream):\n",
    "    \"\"\"\n",
    "    Stream tokens from each source\n",
    "    \"\"\"\n",
    "    def __iter__(self):\n",
    "        rows = super(SourceStream, self).__iter__()\n",
    "        source_sentences = groupby(rows, lambda x: x[0])\n",
    "        \n",
    "        for source, sentences in source_sentences:\n",
    "            yield [word for id, sentence in sentences for word in sentence]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll load in the noun phrase n-grams from last time, limiting ourselves to those that appear at least 0 times in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "with open('../intermediate/noun_ngrams_50k.pkl', 'rb') as infile:\n",
    "    noun_ngrams = pickle.load(infile)\n",
    "\n",
    "noun_ngrams = [n for n in noun_ngrams if noun_ngrams[n] > 100]\n",
    "    \n",
    "# the n-grams we've located will now be identified in the stream of text\n",
    "# using the MWETokenizer from nltk\n",
    "src_stream = SourceStream(sqldb=sql_url, query=query,\n",
    "                          ngrams=noun_ngrams, idcol='base_url')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the matrix\n",
    "Now we can create our document-term matrix, . Each source represents a document, and each "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dummy tokenizer since nltk is doing the tokenizing in the stream\n",
    "# we can't do lambda x: x because it is unpicklable\n",
    "def no_tokenizer(x):\n",
    "    return x\n",
    "\n",
    "# in fact, all processing is done, and we just need to place it\n",
    "# in the appropriate data structure and count out n-grams \n",
    "# we'll insist that we only get words that appear across\n",
    "# at least two sources\n",
    "for i in range(1, 6):\n",
    "    src_stream = SourceStream(sqldb=sql_url, query=query,\n",
    "                          ngrams=noun_ngrams, idcol='base_url')\n",
    "    \n",
    "    vectorizer = CountVectorizer(analyzer='word', preprocessor=None,\n",
    "                                 lowercase=False, tokenizer=no_tokenizer,\n",
    "                                 ngram_range=(i, i), min_df=2)\n",
    "    dtm_source = vectorizer.fit_transform(src_stream)\n",
    "\n",
    "    with open('../intermediate/vec_source_{}gram.pkl'.format(i), 'wb') as vecf,\\\n",
    "         open('../intermediate/dtm_source_{}gram.pkl'.format(i), 'wb') as dtmf:\n",
    "        pickle.dump(vectorizer, vecf)\n",
    "        pickle.dump(dtm_source, dtmf, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../intermediate/vec_source_5gram.pkl', 'rb') as vecf,\\\n",
    "     open('../intermediate/dtm_source_5gram.pkl', 'rb') as dtmf:\n",
    "    vectorizer = pickle.load(vecf)\n",
    "    dtm_source = pickle.load(dtmf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll return to this matrix later"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

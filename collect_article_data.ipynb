{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "layout: post\n",
    "title:  \"Alternate Realities: Article Collection\"\n",
    "date:   2016-10-29 20:01:00 -0500\n",
    "categories: partisan media analysis\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sourcing our Data\n",
    "\n",
    "In my [last post]({% post_url pma/2016-10-22-intro %}), I meanderingly outlined what many in the political sphere feel is a major problem afflicting contemporary American politics. Namely, that there is a dangerous interplay between a deeply polarized electorate and the similarly splintered media it consumes. Facts are subordinate to the ideological narrative. I hope that examining the texts produced by the media will reify these claims.\n",
    "\n",
    "To this end, I've decided to crawl through a selection of partisan news sources, as identified by the Facebook study [\"Exposure to ideologically diverse news and opinion on Facebook\"](http://doi.org/10.1126/science.aaa1160)[^1] and the Wall Street Journal's related [Blue Feed, Red Feed](http://graphics.wsj.com/blue-feed-red-feed/) project. The dataset is located [here](https://github.com/jonkeegan/blue-feed-red-feed-sources); the authors have coded sources based on where on their readership self-identifies on the political spectrum[^2]. The dataset is limited to pages with over 100,000 followers, where at least half the site's links fell into the 'very conservative' or 'very liberal' categories during the study period. You can read more about the methodology [here](http://graphics.wsj.com/blue-feed-red-feed/#methodology).\n",
    "\n",
    "Since most websites don't have a unified trove of past articles, I use the post history from each outlet's Facebook page feed. While this choice limits us to articles posted on Facebook, I feel that what we may lose in coverage is made up for in ease of collection.\n",
    "\n",
    "I've created a Python package called [newsarchives](http://github.com/ahoho/news-archives) to perform this task[^4]. It contains two components: \n",
    "* a 'crawler' that relies on the [Facebook Graph API](https://developers.facebook.com/docs/graph-api) to collect links from past posts from each page feed, saving them to a database\n",
    "* an 'archiver' that resolves these links and extracts the text using the terrific [newspaper](http://github.com/XXXXXX) package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting past Facebook Posts\n",
    "First, we'll import the list of sites and go through their post history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H:\\Personal\\Partisan Media Analysis\\newsarchives\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from newsarchives.crawler import FBGraphCrawler\n",
    "from newsarchives.archiver import NewsArchiver\n",
    "\n",
    "page_data = pd.read_csv('./input/included_sources.csv',\n",
    "                        dtype={'fb_id': str}).drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll want to remove some of these pages because they're either paywalled, not primarily political, associated with a political figure rather than a website, or otherwise innappropriate for our ends[^4]. I also included Alex Jones' InfoWars, which I felt was a glaring omission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "excluded_pages = ['Boing Boing',\n",
    "                  'Herman Cain',\n",
    "                  'Senator Ted Cruz',\n",
    "                  'FiveThirtyEight',\n",
    "                  'Gawker',\n",
    "                  'Republican National Committee',\n",
    "                  'Jezebel',\n",
    "                  'MSNBC',\n",
    "                  'National Republican Congressional Committee',\n",
    "                  'Rolling Stone',\n",
    "                  'U.S. Senator Bernie Sanders',\n",
    "                  'The Daily Show',\n",
    "                  'Upworthy',\n",
    "                  'Vox']\n",
    "page_data = page_data[~page_data.name.isin(excluded_pages)]\n",
    "\n",
    "# turn dataframe into dict\n",
    "page_data.link = page_data.link.replace('https://www.facebook.com/|/', '', regex=True)\n",
    "pages = page_data.set_index('link')['fb_id'].to_dict()\n",
    "pages.update({'infowars':'80256732576'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm using a locally-run instance of a postgreSQL database to store the results, but anything other than SQLite will work.\n",
    "\n",
    "Note that we require an access token in order to query the Facebook API. Lucky for us, it's free."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Page for OBAMAAGAINSTCHRISTIANS not found, removing from list\n"
     ]
    }
   ],
   "source": [
    "access_token = '{}|{}'.format(app_id, app_secret)\n",
    "sql_url = 'postgres://postgres:postgres@localhost/articles'\n",
    "crawler = FBGraphCrawler(access_token, sql_url, pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the time being, I've decided to trawl articles dating back to the arguable beginning of the 2016 election cycle: Trump's announcement of his candidacy in June 2015. The below process takes a while, but we can speed it up using `multiprocessing` and leave it running on a remote instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crawler.save_all_page_feeds(through_date = '2015-06-01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what our results look like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>base_url</th>\n",
       "      <th>created_time</th>\n",
       "      <th>link</th>\n",
       "      <th>shares</th>\n",
       "      <th>page_id</th>\n",
       "      <th>page_name</th>\n",
       "      <th>retrieved_on</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>post_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>282024895179874_844179798964378</th>\n",
       "      <td>freebeacon.com</td>\n",
       "      <td>2015-06-18T16:36:31+0000</td>\n",
       "      <td>http://freebeacon.com/issues/south-carolina-go...</td>\n",
       "      <td>131.0</td>\n",
       "      <td>282024895179874</td>\n",
       "      <td>FreeBeacon</td>\n",
       "      <td>Fri Oct 21 08:59:35 2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282024895179874_844171362298555</th>\n",
       "      <td>freebeacon.com</td>\n",
       "      <td>2015-06-18T16:07:38+0000</td>\n",
       "      <td>http://freebeacon.com/national-security/suspec...</td>\n",
       "      <td>28.0</td>\n",
       "      <td>282024895179874</td>\n",
       "      <td>FreeBeacon</td>\n",
       "      <td>Fri Oct 21 08:59:35 2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282024895179874_844161375632887</th>\n",
       "      <td>freebeacon.com</td>\n",
       "      <td>2015-06-18T15:44:15+0000</td>\n",
       "      <td>http://freebeacon.com/national-security/inside...</td>\n",
       "      <td>280.0</td>\n",
       "      <td>282024895179874</td>\n",
       "      <td>FreeBeacon</td>\n",
       "      <td>Fri Oct 21 08:59:35 2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282024895179874_844132088969149</th>\n",
       "      <td>freebeacon.com</td>\n",
       "      <td>2015-06-18T14:04:06+0000</td>\n",
       "      <td>http://freebeacon.com/national-security/manhun...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>282024895179874</td>\n",
       "      <td>FreeBeacon</td>\n",
       "      <td>Fri Oct 21 08:59:35 2016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       base_url              created_time  \\\n",
       "post_id                                                                     \n",
       "282024895179874_844179798964378  freebeacon.com  2015-06-18T16:36:31+0000   \n",
       "282024895179874_844171362298555  freebeacon.com  2015-06-18T16:07:38+0000   \n",
       "282024895179874_844161375632887  freebeacon.com  2015-06-18T15:44:15+0000   \n",
       "282024895179874_844132088969149  freebeacon.com  2015-06-18T14:04:06+0000   \n",
       "\n",
       "                                                                              link  \\\n",
       "post_id                                                                              \n",
       "282024895179874_844179798964378  http://freebeacon.com/issues/south-carolina-go...   \n",
       "282024895179874_844171362298555  http://freebeacon.com/national-security/suspec...   \n",
       "282024895179874_844161375632887  http://freebeacon.com/national-security/inside...   \n",
       "282024895179874_844132088969149  http://freebeacon.com/national-security/manhun...   \n",
       "\n",
       "                                 shares          page_id   page_name  \\\n",
       "post_id                                                                \n",
       "282024895179874_844179798964378   131.0  282024895179874  FreeBeacon   \n",
       "282024895179874_844171362298555    28.0  282024895179874  FreeBeacon   \n",
       "282024895179874_844161375632887   280.0  282024895179874  FreeBeacon   \n",
       "282024895179874_844132088969149     6.0  282024895179874  FreeBeacon   \n",
       "\n",
       "                                             retrieved_on  \n",
       "post_id                                                    \n",
       "282024895179874_844179798964378  Fri Oct 21 08:59:35 2016  \n",
       "282024895179874_844171362298555  Fri Oct 21 08:59:35 2016  \n",
       "282024895179874_844161375632887  Fri Oct 21 08:59:35 2016  \n",
       "282024895179874_844132088969149  Fri Oct 21 08:59:35 2016  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_sql(\"SELECT * FROM fb_posts LIMIT 4\", crawler.sql_engine, index_col='post_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Article Content\n",
    "We want to do a little cleanup before moving to article scraping. We're going to\n",
    "\n",
    " * Remove duplicate urls, choosing the post with the most shares\n",
    " * Only scrape articles from posts where the source is the same as that of the posting page (e.g., we don't want *Washington Post* articles shared by *Mic*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "        SELECT page_id, base_url\n",
    "        FROM (SELECT page_id,\n",
    "                     base_url,\n",
    "                     count(*) as page_url_posts,\n",
    "                     SUM(COUNT(*)) OVER (PARTITION BY page_id) as page_posts\n",
    "              FROM fb_posts\n",
    "              GROUP BY page_id, base_url) post_summary\n",
    "        WHERE page_url_posts / page_posts > 0.75\n",
    "        \"\"\"\n",
    "sites = pd.read_sql(query, na.sql_engine, index_col = 'page_id')\\\n",
    "          .to_dict()['base_url']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* GROUP BY BASE URL, KEEP ONE WITH MOST SHARES\n",
    "* Cut out most obvious dead end sources from data\n",
    "* Cut out sources that are insufficiently partisan\n",
    "* Get rid of duplicates\n",
    "* Keep only sources with enough posts\n",
    "* Keep only sources with enough total shares\n",
    "* Get rid of low wordcount articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarizing our  Data\n",
    "\n",
    "Let's take a look at some summary statistics for the collected data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "535642 total posts\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "        SELECT base_url, regexp_replace(post_id, '_[0-9]+$', '') as fb_id, count(*) as num_posts\n",
    "        FROM articles\n",
    "        GROUP BY base_url, regexp_replace(post_id, '_[0-9]+$', '')\n",
    "        ORDER BY COUNT(*) DESC\n",
    "        \"\"\"\n",
    "post_summary = pd.read_sql(query, crawler.sql_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Number of Posts by Source Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_posts</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>side</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>left</th>\n",
       "      <td>222676.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>326300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total</th>\n",
       "      <td>535642.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       num_posts\n",
       "side            \n",
       "left    222676.0\n",
       "right   326300.0\n",
       "total   535642.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(post_summary, page_data, on='fb_id')[['side', 'num_posts']]\\\n",
    "  .groupby('side')\\\n",
    "  .sum()\\\n",
    "  .set_value('total', 'num_posts', post_summary.num_posts.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Top 10 Pages by Number of Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_posts</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base_url</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>slate.com</th>\n",
       "      <td>19631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dailycaller.com</th>\n",
       "      <td>18866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>washingtonexaminer.com</th>\n",
       "      <td>16229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>breitbart.com</th>\n",
       "      <td>15433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conservativetribune.com</th>\n",
       "      <td>13427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>teaparty.org</th>\n",
       "      <td>13166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>madworldnews.com</th>\n",
       "      <td>12840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bizpacreview.com</th>\n",
       "      <td>12349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>westernjournalism.com</th>\n",
       "      <td>11226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>americanthinker.com</th>\n",
       "      <td>10890</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         num_posts\n",
       "base_url                          \n",
       "slate.com                    19631\n",
       "dailycaller.com              18866\n",
       "washingtonexaminer.com       16229\n",
       "breitbart.com                15433\n",
       "conservativetribune.com      13427\n",
       "teaparty.org                 13166\n",
       "madworldnews.com             12840\n",
       "bizpacreview.com             12349\n",
       "westernjournalism.com        11226\n",
       "americanthinker.com          10890"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_summary.set_index('base_url')[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Words in Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ResourceClosedError",
     "evalue": "This result object does not return rows. It has been closed automatically.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32mC:\\Anaconda2\\envs\\py3k\\lib\\site-packages\\sqlalchemy\\engine\\result.py\u001b[0m in \u001b[0;36m_fetchall_impl\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    914\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcursor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetchall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'fetchall'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceClosedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-3ba27b5e4910>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mSET\u001b[0m \u001b[0mnum_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mregexp_split_to_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marticle_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mE\u001b[0m\u001b[1;34m'\\\\\\W+'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \"\"\"\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mcrawler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetchall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[1;31m#word_counts = pd.read_sql(query, crawler.sql_engine)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Anaconda2\\envs\\py3k\\lib\\site-packages\\sqlalchemy\\engine\\result.py\u001b[0m in \u001b[0;36mfetchall\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    968\u001b[0m             self.connection._handle_dbapi_exception(\n\u001b[1;32m    969\u001b[0m                 \u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 970\u001b[0;31m                 self.cursor, self.context)\n\u001b[0m\u001b[1;32m    971\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetchmany\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Anaconda2\\envs\\py3k\\lib\\site-packages\\sqlalchemy\\engine\\base.py\u001b[0m in \u001b[0;36m_handle_dbapi_exception\u001b[0;34m(self, e, statement, parameters, cursor, context)\u001b[0m\n\u001b[1;32m   1342\u001b[0m                 )\n\u001b[1;32m   1343\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1344\u001b[0;31m                 \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1345\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Anaconda2\\envs\\py3k\\lib\\site-packages\\sqlalchemy\\util\\compat.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb, cause)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Anaconda2\\envs\\py3k\\lib\\site-packages\\sqlalchemy\\engine\\result.py\u001b[0m in \u001b[0;36mfetchall\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    962\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 964\u001b[0;31m             \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_rows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetchall_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    965\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_soft_close\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Anaconda2\\envs\\py3k\\lib\\site-packages\\sqlalchemy\\engine\\result.py\u001b[0m in \u001b[0;36m_fetchall_impl\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    915\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcursor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetchall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_non_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_non_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Anaconda2\\envs\\py3k\\lib\\site-packages\\sqlalchemy\\engine\\result.py\u001b[0m in \u001b[0;36m_non_result\u001b[0;34m(self, default)\u001b[0m\n\u001b[1;32m    920\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_metadata\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m             raise exc.ResourceClosedError(\n\u001b[0;32m--> 922\u001b[0;31m                 \u001b[1;34m\"This result object does not return rows. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    923\u001b[0m                 \u001b[1;34m\"It has been closed automatically.\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m             )\n",
      "\u001b[0;31mResourceClosedError\u001b[0m: This result object does not return rows. It has been closed automatically."
     ]
    }
   ],
   "source": [
    "# Save a new column with word counts\n",
    "query = \"\"\"\n",
    "        ALTER TABLE articles\n",
    "        ADD COLUMN num_words int;\n",
    "        UPDATE articles\n",
    "        SET num_words = array_length(regexp_split_to_array(trim(article_text), E'\\\\\\W+'), 1);          \n",
    "        \"\"\"\n",
    "crawler.sql_engine.execute(query)\n",
    "#word_counts = pd.read_sql(query, crawler.sql_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_counts.plot('bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[^1]: Note that I'm being somewhat misleading in explicitly declaring these sources as partisan: \"alignment is not a measure of media slant; rather, it captures differences in the kind of content shared among a set of [self-identified] partisans, which can include topic matter, framing, and slant.\"\n",
    "\n",
    "[^2]: -2 being \"very liberal\", +2 \"very conservative\".\n",
    "\n",
    "[^3]: The package uses object oriented programming to model what is basically a procedural task, which is not great form. However, I intend to refactor the codebase so that it is extensible and reusable for other tasks.\n",
    "\n",
    "[^4]: Should we desire articles from some of these excluded sources, we can eventually follow the methodology of the Facebook study and screen out \"hard\" stories from \"soft\" ones."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3k]",
   "language": "python",
   "name": "conda-env-py3k-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

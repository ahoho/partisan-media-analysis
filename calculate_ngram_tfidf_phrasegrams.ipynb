{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "layout: post\n",
    "title:  \"Alternate Realities: A First Look at Partisan Word Use\"\n",
    "date:   2016-12-30 11:48:00 -0500\n",
    "categories: partisan media analysis\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partisan Word Use\n",
    "### [In progress]\n",
    "\n",
    "In the prior post, we created a means of reading tokenized data from our database of articles and a list of n-grams (like *supreme_court*) that we'll impose on our data to count certain n-grams as a single token.\n",
    "\n",
    "Now we're going to engage in a preliminary and fairly rudimentary analysis of the text data as it relates to partisanship. In order to do so, we'll look at which n-grams are most characteristic of the left and the right (later looking at sources individually). We'll expect results sort of in line with this [538 piece](http://fivethirtyeight.com/features/these-are-the-phrases-each-gop-candidate-repeats-most/) on the text of the GOP debates.\n",
    "\n",
    "This analysis will also set us up for topic modeling (out of fashion as it may be) since we'll just be building a large document-term matrix. I also hope to use it on weekly cuts of the data to see how emphasis in coverage changed over the election cycle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the Data\n",
    "First, we want to collect the data at the level of each source, which will require us to subclass the `SentenceStream` generator we built in the last post. `CountVectorizer` from `scikit-learn` treats each element in an iterator as a document, so we'll restructure the generator such that all words from each source are combined into one list of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import groupby, imap\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "\n",
    "from stream import SentenceStream #from the prior post\n",
    "\n",
    "sql_url = 'postgres://postgres:**password**@localhost/articles'\n",
    "\n",
    "# ordering is a necessity, else groupby won't work\n",
    "# we also have a few strange base_url's with '{xyz.com}'\n",
    "query = \"\"\"\n",
    "        SELECT base_url, article_text\n",
    "        FROM articles\n",
    "        WHERE num_words > 100\n",
    "        ORDER BY base_url\n",
    "        \"\"\"\n",
    "\n",
    "class SourceNGramStream(SentenceStream):\n",
    "    \"\"\"\n",
    "    Get a stream of pre-identified n-grams from each source\n",
    "    \"\"\"\n",
    "    def __iter__(self):\n",
    "        rows = super(SourceNGramStream, self).__iter__()\n",
    "        source_sentences = groupby(rows, lambda x: x[0])\n",
    "        \n",
    "        for source, sentences in source_sentences:\n",
    "            source_ngrams = [word for id, sentence in sentences for word in sentence if '_' in word]\n",
    "            if source_ngrams:\n",
    "                yield source_ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll load in the identified n-grams from last time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "with open('../intermediate/phrasegrams_all.pkl', 'rb') as infile:\n",
    "    ngrams = pickle.load(infile)\n",
    "\n",
    "# the n-grams we've located will now be identified in the stream of text\n",
    "# using the MWETokenizer from nltk\n",
    "src_stream = SourceNGramStream(sqldb=sql_url, query=query,\n",
    "                                ngrams=ngrams, idcol='base_url')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a document-term matrix\n",
    "Now we can create our document-term matrix, where each source represents a document and our columns will be a mix of single tokens and the n-grams from earlier.\n",
    "\n",
    "Impoortantly, we're going to limit ourselves to tokens and n-grams that appear in two or more sources. I believe this choice enables us to consider these sources as a network, where there might be patterns of mututal influence on rhetoric and thinking. We don't really care about one-off uses of a particular word or phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'src_stream' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-1f3837ef1040>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m                              \u001b[0mlowercase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mno_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                              min_df=2)\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdtm_source\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_stream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'src_stream' is not defined"
     ]
    }
   ],
   "source": [
    "# dummy tokenizer since nltk is doing the tokenizing in the stream\n",
    "# we can't do lambda x: x because it is unpicklable\n",
    "def no_tokenizer(x):\n",
    "    return x\n",
    "\n",
    "# in fact, all processing is done, and we just need to place it\n",
    "# in the appropriate data structure \n",
    "vectorizer = CountVectorizer(analyzer='word', preprocessor=None,\n",
    "                             lowercase=False, tokenizer=no_tokenizer,\n",
    "                             min_df=2)\n",
    "dtm_source = vectorizer.fit_transform(src_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import io\n",
    "\n",
    "with open('../intermediate/vec_source_phrasegram.pkl', 'wb') as vecf:\n",
    "    pickle.dump(vectorizer, vecf)\n",
    "io.mmwrite('../intermediate/dtm_source_phrasegram.mtx',  dtm_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import io\n",
    "\n",
    "def no_tokenizer(x):\n",
    "    return x\n",
    "\n",
    "import cPickle as pickle\n",
    "with open('../intermediate/vec_source_phrasegram.pkl', 'rb') as vecf:\n",
    "    vectorizer = pickle.load(vecf)\n",
    "    dtm_source = io.mmread('../intermediate/dtm_source_phrasegram.mtx')\n",
    "    dtm_source = dtm_source_raw.tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite having limited ourselves to only n-grams that appear in more than one source, there are still some phrases that muddy up the waters unecessarily (like 'washington examiner news desk'). As a result, we'll remove those that appear in only one source over 95% of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "idx_norm_terms = np.all(np.true_divide(dtm_source.toarray(), dtm_source.sum(axis=0)) <= 0.95, axis=0).A[0]\n",
    "features = vectorizer.get_feature_names()\n",
    "features = [f for i, f in enumerate(features) if idx_norm_terms[i]]\n",
    "dtm_source = csr_matrix(dtm_source[:,idx_norm_terms])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Look at the Left and Right\n",
    "We'll return to this source-based matrix later, and will for now collapse this matrix to only two rows, representing left- and right- aligned sources for a more straightforward analysis of which ideas are of greatest concern to each side of the aisle.\n",
    "\n",
    "We need to pull the old alignment data from [\"Blue Feed, Red Feed\"](https://github.com/jonkeegan/blue-feed-red-feed-sources) to correctly identify who is on the left and the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "# same order as earlier query\n",
    "source_query = \"\"\"\n",
    "               SELECT base_url,\n",
    "                      split_part(post_id, '_', 1) as fb_id\n",
    "               FROM articles \n",
    "               WHERE num_words > 100 \n",
    "               GROUP BY base_url,\n",
    "                        split_part(post_id, '_', 1) \n",
    "               ORDER BY base_url\n",
    "               \"\"\"\n",
    "\n",
    "# collect source alignment data\n",
    "sources = pd.read_sql(source_query, create_engine(sql_url))  \n",
    "alignment_data = pd.read_csv('./input/included_sources.csv', dtype={'fb_id':object})\n",
    "sources = sources.merge(alignment_data, how='left')\n",
    "\n",
    "# we supplemented the data with infowars\n",
    "sources.loc[sources.base_url == 'infowars.com', 'side'] = 'right'\n",
    "\n",
    "# get indexes of left and right sources\n",
    "sources_left = np.where(sources.side == 'left')[0]\n",
    "sources_right = np.where(sources.side == 'right')[0]\n",
    "\n",
    "# create a new document-term matrix of 2 rows\n",
    "dtm_side = csr_matrix(np.append(dtm_source[sources_left,:].sum(axis=0),\n",
    "                                dtm_source[sources_right,:].sum(axis=0),\n",
    "                                axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 31085)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm_side.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And belhold, our matrix! We'll now transform this count matrix (where A<sub>ij</sub> is the number of times term j appears on side i) into a [normalized term-frequency matrix](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html). This means that phrases that are common to each side are discounted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dtm_side_tfidf = TfidfTransformer().fit_transform(dtm_side)\n",
    "\n",
    "#get the column indices from largest to smallest\n",
    "idx_sorted_tfidf_left = np.argsort(dtm_side_tfidf[0, ].toarray()[0])[::-1]\n",
    "idx_sorted_tfidf_right = np.argsort(dtm_side_tfidf[1, ].toarray()[0])[::-1]\n",
    "\n",
    "#nonzero terms\n",
    "terms_sorted_tfidf_left = [features[i] for i in idx_sorted_tfidf_left if dtm_side[0,i]] \n",
    "terms_sorted_tfidf_right = [features[i] for i in idx_sorted_tfidf_right if dtm_side[1,i]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1000x84 sparse matrix of type '<type 'numpy.int32'>'\n",
       "\twith 67201 stored elements in Compressed Sparse Column format>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm_source[:,idx_sorted_tfidf_left[:1000]].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's put together this information and plot it\n",
    "pd.DataFrame(dtm_source[:,idx_sorted_tfidf_left[:1000]].T.toarray(),\n",
    "             columns=sources.base_url)\\\n",
    "  .assign(terms=terms_sorted_tfidf_left[:1000],\n",
    "          tfidf=dtm_side_tfidf[0, idx_sorted_tfidf_left.toarray()])\\\n",
    "  .to_csv('./output/ngrams_tfidf_top1000_left.csv', index_label='rank')\n",
    "\n",
    "pd.DataFrame(dtm_source[:,idx_sorted_tfidf_right[:1000]].T.toarray(),\n",
    "             columns=sources.base_url)\\\n",
    "  .assign(terms=terms_sorted_tfidf_right[:1000],\n",
    "          tfidf=dtm_side_tfidf[1, idx_sorted_tfidf_right].toarray())\\\n",
    "  .to_csv('./output/ngrams_tfidf_top1000_right.csv', index_label='rank')\n",
    "\n",
    "sources[['base_url', 'fb_id', 'side', 'avg_align']].to_csv('./output/source_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'feel_like',\n",
       " u'20_years',\n",
       " u'new_orleans',\n",
       " u'first_place',\n",
       " u'yet_another',\n",
       " u'every_year',\n",
       " u'vast_majority',\n",
       " u'rights_reserved',\n",
       " u'let_us',\n",
       " u'least_one',\n",
       " u'police_brutality',\n",
       " u'material_may',\n",
       " u'rewritten_or_redistributed',\n",
       " u'good_news',\n",
       " u'lgbt_community',\n",
       " u'90_percent',\n",
       " u'months_ago',\n",
       " u'three_times',\n",
       " u'weeks_ago',\n",
       " u'ever_since',\n",
       " u'campaign_trail',\n",
       " u'north_dakota',\n",
       " u'federal_judge',\n",
       " u'juan_gonzlez',\n",
       " u'many_ways',\n",
       " u'black_lives_matter_movement',\n",
       " u'take_place',\n",
       " u'white_men',\n",
       " u'weve_seen',\n",
       " u'much_less',\n",
       " u'take_care',\n",
       " u'get_rid',\n",
       " u'republican_national_convention',\n",
       " u'60_percent',\n",
       " u'election_day',\n",
       " u'past_year',\n",
       " u'los_angeles_times',\n",
       " u'feels_like',\n",
       " u'next_day',\n",
       " u'much_better',\n",
       " u'black_community',\n",
       " u'great_deal',\n",
       " u'american_politics',\n",
       " u'looked_like',\n",
       " u'lgbt_rights',\n",
       " u'days_later',\n",
       " u'white_supremacy',\n",
       " u'take_action',\n",
       " u'making_sure',\n",
       " u'whole_thing',\n",
       " u'move_forward',\n",
       " u'makes_sense',\n",
       " u'final_form',\n",
       " u'rush_transcript',\n",
       " u'make_sense',\n",
       " u'fossil_fuel_industry',\n",
       " u'republican_voters',\n",
       " u'copy_may',\n",
       " u'white_supremacists',\n",
       " u'police_violence',\n",
       " u'presidential_candidate_donald_trump',\n",
       " u'tells_us',\n",
       " u'10_million',\n",
       " u'bill_cosby',\n",
       " u'long_ago',\n",
       " u'100_million',\n",
       " u'common_dreams',\n",
       " u'public_education',\n",
       " u'voting_rights_act',\n",
       " u'public_safety',\n",
       " u'doesnt_matter',\n",
       " u'thus_far',\n",
       " u'large_part',\n",
       " u'bad_news',\n",
       " u'whole_lot',\n",
       " u'capitol_hill',\n",
       " u'study_published',\n",
       " u'four_times',\n",
       " u'democratic_nomination',\n",
       " u'constitutional_rights',\n",
       " u'put_together',\n",
       " u'look_forward',\n",
       " u'ive_seen',\n",
       " u'senator_sanders',\n",
       " u'trans_women',\n",
       " u'sandra_bland',\n",
       " u'much_money',\n",
       " u'friday_night',\n",
       " u'radio_show',\n",
       " u'oval_office',\n",
       " u'political_revolution',\n",
       " u'new_york_state',\n",
       " u'nicki_minaj',\n",
       " u'next_month',\n",
       " u'el_nio',\n",
       " u'national_anthem',\n",
       " u'nearly_every',\n",
       " u'sen_sanders',\n",
       " u'college_students',\n",
       " u'take_advantage']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms_sorted_tfidf_left[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'eligible_news_publisher',\n",
       " u'available_without_charge',\n",
       " u'daily_caller_news_foundation',\n",
       " u'original_content_please_contact_email_protected',\n",
       " u'large_audience',\n",
       " u'licensing_opportunities',\n",
       " u'aborted_babies',\n",
       " u'daily_signal',\n",
       " u'democrat_party',\n",
       " u'islamic_state_group',\n",
       " u'privacy_policy',\n",
       " u'share_your_thoughts',\n",
       " u'latest_video_at_videofoxnewscom',\n",
       " u'comments_section',\n",
       " u'interesting_story',\n",
       " u'open_borders',\n",
       " u'bear_arms',\n",
       " u'gop_establishment',\n",
       " u'terrorist_groups',\n",
       " u'sanctuary_cities',\n",
       " u'told_wnd',\n",
       " u'texas_sen_ted_cruz',\n",
       " u'refugee_resettlement',\n",
       " u'reprinted_with_permission',\n",
       " u'taxpayer_dollars',\n",
       " u'mr_obama',\n",
       " u'gop_front-runner',\n",
       " u'contested_convention',\n",
       " u'pro-life_movement']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[t for t in terms_sorted_tfidf_right[:100] if t not in terms_sorted_tfidf_left[:500]]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

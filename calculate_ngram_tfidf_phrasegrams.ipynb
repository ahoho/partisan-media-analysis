{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "layout: post\n",
    "title:  \"Alternate Realities: A First Look at Partisan Word Use\"\n",
    "date:   2016-12-30 11:48:00 -0500\n",
    "categories: partisan media analysis\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partisan Word Use\n",
    "### [In progress]\n",
    "\n",
    "In the prior post, we created a means of reading tokenized data from our database of articles and a list of n-grams (like *supreme_court*) that we'll impose on our data to count certain n-grams as a single token.\n",
    "\n",
    "In this post, we're going to engage in a preliminary and fairly rudimentary analysis of the text data as it relates to partisanship. In order to do so, we'll look at which n-grams are most characteristic of the left and the right (later looking at sources individually, perhaps at a future date). I'm expecting results sort of in line with this [538 piece](http://fivethirtyeight.com/features/these-are-the-phrases-each-gop-candidate-repeats-most/) on the text of the GOP debates.\n",
    "\n",
    "This analysis will also set us up for topic modeling (out of fashion as it may be) since we'll just be building a large document-term matrix. I also hope to use it on weekly cuts of the data to see how emphasis in coverage changed over the election cycle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the Data\n",
    "First, we want to collect the data at the level of each source, which will require us to subclass the `SentenceStream` generator we built in the last post. `CountVectorizer` from `scikit-learn` treats each element in an iterator as a document, so we'll restructure the generator such that all words from each source are combined into one list of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import groupby, imap\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from stream import SentenceStream #from the prior post\n",
    "\n",
    "sql_url = 'postgres://postgres:**PASSWORD**@localhost/articles'\n",
    "\n",
    "# ordering is a necessity, else groupby won't work\n",
    "# we also have a few strange base_url's with '{xyz.com}',\n",
    "# as well as reprinted articles from The Daily Caller we'll remove\n",
    "query = \"\"\"\n",
    "        SELECT base_url, article_text\n",
    "        FROM articles\n",
    "        WHERE num_words > 100 and not\n",
    "              (lower(article_text) like '%%daily caller news foundation%%' and\n",
    "               base_url != 'dailycaller.com') and not\n",
    "               lower(article_text) like '%%copyright 20__ the associated press%%'\n",
    "        ORDER BY base_url\n",
    "        \"\"\"\n",
    "\n",
    "class SourceNGramStream(SentenceStream):\n",
    "    \"\"\"\n",
    "    Get a stream of pre-identified n-grams from each source\n",
    "    \"\"\"\n",
    "    def __iter__(self):\n",
    "        rows = super(SourceNGramStream, self).__iter__()\n",
    "        source_sentences = groupby(rows, lambda x: x[0])\n",
    "        \n",
    "        for source, sentences in source_sentences:\n",
    "            source_ngrams = [word for id, sentence in sentences for word in sentence if '_' in word]\n",
    "            if source_ngrams:\n",
    "                yield source_ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll load in the identified n-grams from last time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "with open('../intermediate/phrasegrams_all.pkl', 'rb') as infile:\n",
    "    ngrams = pickle.load(infile)\n",
    "\n",
    "# the n-grams we've located will now be identified in the stream of text\n",
    "# using the MWETokenizer from nltk\n",
    "src_stream = SourceNGramStream(sqldb=sql_url, query=query,\n",
    "                               ngrams=ngrams, idcol='base_url')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a document-term matrix\n",
    "Now we can create a document-term matrix, where each source represents a document and our columns will be the n-grams from earlier.\n",
    "\n",
    "Impoortantly, we're going to limit ourselves to tokens and n-grams that appear in two or more sources. I believe this choice enables us to consider these sources as a network, where there might be patterns of mututal influence on rhetoric and thinking. We don't really care about one-off uses of a particular word or phrase. (It also has the nice side-effect of getting rid of a lot of junk phrases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we use this dummy tokenizer since nltk is doing the tokenizing in the stream\n",
    "# (we can't do lambda x: x because it is unpicklable)\n",
    "def no_tokenizer(x):\n",
    "    return x\n",
    "\n",
    "# in fact, all processing is done, and we just need to place it\n",
    "# in the appropriate data structure \n",
    "vectorizer = CountVectorizer(analyzer='word', preprocessor=None,\n",
    "                             lowercase=False, tokenizer=no_tokenizer,\n",
    "                             min_df=2)\n",
    "dtm_source = vectorizer.fit_transform(src_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import io\n",
    "\n",
    "with open('../intermediate/vec_source_phrasegram.pkl', 'wb') as vecf:\n",
    "    pickle.dump(vectorizer, vecf)\n",
    "io.mmwrite('../intermediate/dtm_source_phrasegram.mtx',  dtm_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite having limited ourselves to only n-grams that appear in more than one source, there are still some phrases that muddy up the waters unecessarily (like 'washington examiner news desk'). As a result, we'll remove those that appear in only one source over 95% of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "idx_norm_terms = np.all(np.true_divide(dtm_source.toarray(), dtm_source.sum(axis=0)) <= 0.95, axis=0).A[0]\n",
    "features = vectorizer.get_feature_names()\n",
    "features = [f for i, f in enumerate(features) if idx_norm_terms[i]]\n",
    "dtm_source = csr_matrix(dtm_source[:,idx_norm_terms])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An Initial Examination of Sources on the Left and Right\n",
    "We'll return to this source-based matrix later, and will for now collapse this matrix to two rows, representing left- and right- aligned sources. This enables a more straightforward analysis of which ideas are of greatest concern to each side of the aisle.\n",
    "\n",
    "We need to pull the old alignment data from [\"Blue Feed, Red Feed\"](https://github.com/jonkeegan/blue-feed-red-feed-sources) to correctly identify who is on the left and the right, then sum up the rows of our source matrix, grouping by alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "\n",
    "# same order as earlier query\n",
    "source_query = \"\"\"\n",
    "               SELECT base_url,\n",
    "                      split_part(post_id, '_', 1) as fb_id\n",
    "               FROM articles \n",
    "               WHERE num_words > 100\n",
    "               GROUP BY base_url,\n",
    "                        split_part(post_id, '_', 1) \n",
    "               ORDER BY base_url\n",
    "               \"\"\"\n",
    "\n",
    "# collect source alignment data\n",
    "sources = pd.read_sql(source_query, create_engine(sql_url))  \n",
    "alignment_data = pd.read_csv('./input/included_sources.csv', dtype={'fb_id':object})\\\n",
    "                   .drop_duplicates('fb_id')\n",
    "sources = sources.merge(alignment_data, how='left')\n",
    "\n",
    "# we supplemented the data with infowars\n",
    "sources.loc[sources.base_url == 'infowars.com', 'side'] = 'right'\n",
    "\n",
    "# get indexes of left and right sources\n",
    "sources_left = np.where(sources.side == 'left')[0]\n",
    "sources_right = np.where(sources.side == 'right')[0]\n",
    "\n",
    "# create a new document-term matrix of 2 rows\n",
    "dtm_side = csr_matrix(np.append(dtm_source[sources_left,:].sum(axis=0),\n",
    "                                dtm_source[sources_right,:].sum(axis=0),\n",
    "                                axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x46945 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 92574 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm_side"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And behold, our matrix! We'll now transform this count matrix (where A<sub>ij</sub> is the number of times term j appears on side i) into a [normalized term-frequency matrix](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html). This means that phrases that are highly common to each side are discounted, therefore promoting those that are more distinctive to each political side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dtm_side_tfidf = TfidfTransformer().fit_transform(dtm_side)\n",
    "\n",
    "#get the column indices from largest to smallest\n",
    "idx_sorted_tfidf_left = np.argsort(dtm_side_tfidf[0, ].toarray()[0])[::-1]\n",
    "idx_sorted_tfidf_right = np.argsort(dtm_side_tfidf[1, ].toarray()[0])[::-1]\n",
    "\n",
    "#nonzero terms\n",
    "terms_sorted_tfidf_left = [features[i] for i in idx_sorted_tfidf_left if dtm_side[0,i]] \n",
    "terms_sorted_tfidf_right = [features[i] for i in idx_sorted_tfidf_right if dtm_side[1,i]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's put together this information for plotting\n",
    "pd.DataFrame({'term': terms_sorted_tfidf_left[:10000],\n",
    "              'tfidf': dtm_side_tfidf[0, idx_sorted_tfidf_left[:10000]].A[0],\n",
    "              'side': 'left'})\\\n",
    "  .to_csv('./output/ngrams_top10000_left.csv', index_label='rank')\n",
    "    \n",
    "pd.DataFrame({'term': terms_sorted_tfidf_right[:10000],\n",
    "              'tfidf': dtm_side_tfidf[1, idx_sorted_tfidf_right[:10000]].A[0],\n",
    "              'side': 'right'})\\\n",
    "  .to_csv('./output/ngrams_top10000_right.csv', index_label='rank')\n",
    "\n",
    "pd.DataFrame(dtm_source.T.toarray(),\n",
    "             columns=sources.base_url)\\\n",
    "  .assign(term=features)\\\n",
    "  .to_csv('./output/dtm_source_counts.csv', index=False)\n",
    "\n",
    "sources[['base_url', 'fb_id', 'side', 'avg_align']].to_csv('./output/source_info.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Data\n",
    "In a separate R script, I've summarized the above data to determine which n-grams are most associated with each side. In order to do accomplish this task, I sum up the number of times each n-gram appears in sources on the left and the right (normalizing for the total word count on each side), then find the relative share that each term appears in right-aligned sources. \n",
    "\n",
    "$$ \\sum_{t=1}^n \\frac{tf_{t, r}}{tf_{t, r} + tf_{t, l}} $$\n",
    "\n",
    "This constitutes a measure of term partisanship, the distribution of which is displayed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "![Plot]({{ site.url }}/assets/term_alignment_dist.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that established, we can take a look at the top terms on each side, as ranked by tf-idf. The size and color of these n-grams corrseponds to the above measure. Since many terms appear on both the left and right, I've also created a filtered version of the data where we only look at terms with a partisanship measure greater than 1 standard deviation beyond the mean (per the chart above).\n",
    "\n",
    "While this is only an exploratory look at the data, I think we're starting to see some interesting results (despite the obvious junk). *islamic_state* features more heavily on the right than on the left, for example.\n",
    "\n",
    "If I had to make an initial and wholly speculative interpretation, I'd say that these data support the idea that hyperpartisan news sources focus on threats to their respective ideologies and values, rather than establishing them from first principles (or, if they are engaging in a constructive mode, it is in a negative sense: \"we're against\" rather than \"we're for\"). I guess this is unsurprising, but I think we now have some hard evidence corroborating what we already knew (such is the bane of [positivist disciplines](https://tni-back-soon.github.io/essays/podcast-out/), apparently).\n",
    "\n",
    "But just look: the left talks about *gun_violence* and the right *gun_control*. Conservative media focuses on *illegal_immigrants*, *syrian_refuges*, *sharia_law*, and *islamic_terrorism*. The left is concerned with the *religious_right*, *white_supremacist*, and *wall_street*. To be sure, the left media does have terms aligned with values and reform, like *mass_incarceration*, *climate_change*, and *human_rights* (as well as *lgbt_rights*). \n",
    "\n",
    "But it's also my bias that I view *climate_change* as non-reactionary. Using sentiment analysis is a reasonable future step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "{% include term_viz.html %}"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

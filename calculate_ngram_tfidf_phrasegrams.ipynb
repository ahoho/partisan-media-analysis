{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "layout: post\n",
    "title:  \"Alternate Realities: A First Look at Partisan Word Use\"\n",
    "date:   2016-12-30 11:48:00 -0500\n",
    "categories: partisan media analysis\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partisan Word Use\n",
    "### [In progress]\n",
    "\n",
    "In the prior post, we created a means of reading tokenized data from our database of articles and a list of n-grams (like *supreme_court*) that we'll impose on our data to count certain n-grams as a single token.\n",
    "\n",
    "Now we're going to engage in a preliminary and fairly rudimentary analysis of the text data as it relates to partisanship. In order to do so, we'll look at which n-grams are most characteristic of the left and the right (later looking at sources individually). We'll expect results sort of in line with this [538 piece](http://fivethirtyeight.com/features/these-are-the-phrases-each-gop-candidate-repeats-most/) on the text of the GOP debates.\n",
    "\n",
    "This analysis will also set us up for topic modeling (out of fashion as it may be) since we'll just be building a large document-term matrix. I also hope to use it on weekly cuts of the data to see how emphasis in coverage changed over the election cycle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Document-Term-Matrix\n",
    "\n",
    "#### Reading the Data\n",
    "First, we want to collect the data at the level of each source, which will require us to subclass the `SentenceStream` generator we built in the last post. `CountVectorizer` from `scikit-learn` treats each element in an iterator as a document, so we'll restructure the generator such that all words from each source are combined into one list of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import groupby, imap\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "\n",
    "from stream import SentenceStream #from the prior post\n",
    "\n",
    "sql_url = 'postgres://postgres:Pluto2015!@localhost/articles'\n",
    "\n",
    "# ordering is a necessity, else groupby won't work\n",
    "query = \"\"\"\n",
    "        SELECT base_url, article_text\n",
    "        FROM articles\n",
    "        WHERE num_words > 100\n",
    "        ORDER BY base_url\n",
    "        \"\"\"\n",
    "\n",
    "class SourceStream(SentenceStream):\n",
    "    \"\"\"\n",
    "    Stream tokens from each source\n",
    "    \"\"\"\n",
    "    def __iter__(self):\n",
    "        rows = super(SourceStream, self).__iter__()\n",
    "        source_sentences = groupby(rows, lambda x: x[0])\n",
    "        \n",
    "        for source, sentences in source_sentences:\n",
    "            yield [word for id, sentence in sentences for word in sentence]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll load in the identified n-grams from last time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "with open('../intermediate/phrasegrams_all.pkl', 'rb') as infile:\n",
    "    ngrams = pickle.load(infile)\n",
    "    \n",
    "# the n-grams we've located will now be identified in the stream of text\n",
    "# using the MWETokenizer from nltk\n",
    "src_stream = SourceStream(sqldb=sql_url, query=query,\n",
    "                          ngrams=ngrams, idcol='base_url')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the matrix\n",
    "Now we can create our document-term matrix, where each source represents a document and our columns will be a mix of single tokens and the n-grams from earlier.\n",
    "\n",
    "Impoortantly, we're going to limit ourselves to tokens and n-grams that appear in two or more sources. I believe this choice enables us to consider these sources as a network, where there might be patterns of mututal influence on rhetoric and thinking. We don't really care about one-off uses of a particular word or phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dummy tokenizer since nltk is doing the tokenizing in the stream\n",
    "# we can't do lambda x: x because it is unpicklable\n",
    "def no_tokenizer(x):\n",
    "    return x\n",
    "\n",
    "# in fact, all processing is done, and we just need to place it\n",
    "# in the appropriate data structure \n",
    "\n",
    "# we'll insist that we only get words that appear across\n",
    "# at least two sources\n",
    "vectorizer = CountVectorizer(analyzer='word', preprocessor=None,\n",
    "                             lowercase=False, tokenizer=no_tokenizer,\n",
    "                             min_df=2)\n",
    "dtm_source = vectorizer.fit_transform(src_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll return to this source-based matrix later, and will for now collapse this matrix to only two rows, representing left- and right- aligned sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

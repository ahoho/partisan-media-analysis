{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "layout: post\n",
    "title:  \"Alternate Realities: A First Look at Partisan Word Use\"\n",
    "date:   2016-12-30 11:48:00 -0500\n",
    "categories: partisan media analysis\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partisan Word Use\n",
    "### [In progress]\n",
    "\n",
    "In the prior post, we created a means of reading tokenized data from our database of articles and a list of n-grams (like *supreme_court*) that we'll impose on our data to count certain n-grams as a single token.\n",
    "\n",
    "Now we're going to engage in a preliminary and fairly rudimentary analysis of the text data as it relates to partisanship. In order to do so, we'll look at which n-grams are most characteristic of the left and the right (later looking at sources individually). We'll expect results sort of in line with this [538 piece](http://fivethirtyeight.com/features/these-are-the-phrases-each-gop-candidate-repeats-most/) on the text of the GOP debates.\n",
    "\n",
    "This analysis will also set us up for topic modeling (out of fashion as it may be) since we'll just be building a large document-term matrix. I also hope to use it on weekly cuts of the data to see how emphasis in coverage changed over the election cycle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the Data\n",
    "First, we want to collect the data at the level of each source, which will require us to subclass the `SentenceStream` generator we built in the last post. `CountVectorizer` from `scikit-learn` treats each element in an iterator as a document, so we'll restructure the generator such that all words from each source are combined into one list of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import groupby, imap\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "\n",
    "from stream import SentenceStream #from the prior post\n",
    "\n",
    "sql_url = 'postgres://postgres:**PASSWORD**@localhost/articles'\n",
    "\n",
    "# ordering is a necessity, else groupby won't work\n",
    "# we also have a few strange base_url's with '{xyz.com}',\n",
    "# as well as reprinted articles from The Daily Caller we'll remove\n",
    "query = \"\"\"\n",
    "        SELECT base_url, article_text\n",
    "        FROM articles\n",
    "        WHERE num_words > 100 and not\n",
    "              (lower(article_text) like '%%daily caller news foundation%%' and\n",
    "               base_url != 'dailycaller.com') and not\n",
    "               lower(article_text) like '%%copyright 20__ the associated press%%'\n",
    "        ORDER BY base_url\n",
    "        \"\"\"\n",
    "\n",
    "class SourceNGramStream(SentenceStream):\n",
    "    \"\"\"\n",
    "    Get a stream of pre-identified n-grams from each source\n",
    "    \"\"\"\n",
    "    def __iter__(self):\n",
    "        rows = super(SourceNGramStream, self).__iter__()\n",
    "        source_sentences = groupby(rows, lambda x: x[0])\n",
    "        \n",
    "        for source, sentences in source_sentences:\n",
    "            source_ngrams = [word for id, sentence in sentences for word in sentence if '_' in word]\n",
    "            if source_ngrams:\n",
    "                yield source_ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll load in the identified n-grams from last time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "with open('../intermediate/phrasegrams_all.pkl', 'rb') as infile:\n",
    "    ngrams = pickle.load(infile)\n",
    "\n",
    "# the n-grams we've located will now be identified in the stream of text\n",
    "# using the MWETokenizer from nltk\n",
    "src_stream = SourceNGramStream(sqldb=sql_url, query=query,\n",
    "                               ngrams=ngrams, idcol='base_url')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a document-term matrix\n",
    "Now we can create our document-term matrix, where each source represents a document and our columns will be a mix of single tokens and the n-grams from earlier.\n",
    "\n",
    "Impoortantly, we're going to limit ourselves to tokens and n-grams that appear in two or more sources. I believe this choice enables us to consider these sources as a network, where there might be patterns of mututal influence on rhetoric and thinking. We don't really care about one-off uses of a particular word or phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we use this dummy tokenizer since nltk is doing the tokenizing in the stream\n",
    "# (we can't do lambda x: x because it is unpicklable)\n",
    "def no_tokenizer(x):\n",
    "    return x\n",
    "\n",
    "# in fact, all processing is done, and we just need to place it\n",
    "# in the appropriate data structure \n",
    "vectorizer = CountVectorizer(analyzer='word', preprocessor=None,\n",
    "                             lowercase=False, tokenizer=no_tokenizer,\n",
    "                             min_df=2)\n",
    "dtm_source = vectorizer.fit_transform(src_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import io\n",
    "\n",
    "with open('../intermediate/vec_source_phrasegram.pkl', 'wb') as vecf:\n",
    "    pickle.dump(vectorizer, vecf)\n",
    "io.mmwrite('../intermediate/dtm_source_phrasegram.mtx',  dtm_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import io\n",
    "\n",
    "def no_tokenizer(x):\n",
    "    return x\n",
    "\n",
    "import cPickle as pickle\n",
    "with open('../intermediate/vec_source_phrasegram.pkl', 'rb') as vecf:\n",
    "    vectorizer = pickle.load(vecf)\n",
    "    \n",
    "dtm_source = io.mmread('../intermediate/dtm_source_phrasegram.mtx')\n",
    "dtm_source = dtm_source.tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite having limited ourselves to only n-grams that appear in more than one source, there are still some phrases that muddy up the waters unecessarily (like 'washington examiner news desk'). As a result, we'll remove those that appear in only one source over 95% of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "idx_norm_terms = np.all(np.true_divide(dtm_source.toarray(), dtm_source.sum(axis=0)) <= 0.95, axis=0).A[0]\n",
    "features = vectorizer.get_feature_names()\n",
    "features = [f for i, f in enumerate(features) if idx_norm_terms[i]]\n",
    "dtm_source = csr_matrix(dtm_source[:,idx_norm_terms])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An Initial Examination of the Left and Right\n",
    "We'll return to this source-based matrix later, and will for now collapse this matrix to only two rows, representing left- and right- aligned sources. This enables a more straightforward analysis of which ideas are of greatest concern to each side of the aisle.\n",
    "\n",
    "We need to pull the old alignment data from [\"Blue Feed, Red Feed\"](https://github.com/jonkeegan/blue-feed-red-feed-sources) to correctly identify who is on the left and the right, then sum up the rows of our source matrix, grouping by alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sql_url' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-53f6a0c25d56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# collect source alignment data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0msources\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_sql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_query\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0malignment_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./input/included_sources.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'fb_id'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m                   \u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fb_id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0msources\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msources\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malignment_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sql_url' is not defined"
     ]
    }
   ],
   "source": [
    "sql_url = \n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "# same order as earlier query\n",
    "source_query = \"\"\"\n",
    "               SELECT base_url,\n",
    "                      split_part(post_id, '_', 1) as fb_id\n",
    "               FROM articles \n",
    "               WHERE num_words > 100\n",
    "               GROUP BY base_url,\n",
    "                        split_part(post_id, '_', 1) \n",
    "               ORDER BY base_url\n",
    "               \"\"\"\n",
    "\n",
    "# collect source alignment data\n",
    "sources = pd.read_sql(source_query, create_engine(sql_url))  \n",
    "alignment_data = pd.read_csv('./input/included_sources.csv', dtype={'fb_id':object})\\\n",
    "                   .drop_duplicates('fb_id')\n",
    "sources = sources.merge(alignment_data, how='left')\n",
    "\n",
    "# we supplemented the data with infowars\n",
    "sources.loc[sources.base_url == 'infowars.com', 'side'] = 'right'\n",
    "\n",
    "# get indexes of left and right sources\n",
    "sources_left = np.where(sources.side == 'left')[0]\n",
    "sources_right = np.where(sources.side == 'right')[0]\n",
    "\n",
    "# create a new document-term matrix of 2 rows\n",
    "dtm_side = csr_matrix(np.append(dtm_source[sources_left,:].sum(axis=0),\n",
    "                                dtm_source[sources_right,:].sum(axis=0),\n",
    "                                axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x46945 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 92574 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm_side"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And behold, our matrix! We'll now transform this count matrix (where A<sub>ij</sub> is the number of times term j appears on side i) into a [normalized term-frequency matrix](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html). This means that phrases that are highly common to each side are discounted, therefore promoting those that are more distinctive to each political side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dtm_side_tfidf = TfidfTransformer().fit_transform(dtm_side)\n",
    "\n",
    "#get the column indices from largest to smallest\n",
    "idx_sorted_tfidf_left = np.argsort(dtm_side_tfidf[0, ].toarray()[0])[::-1]\n",
    "idx_sorted_tfidf_right = np.argsort(dtm_side_tfidf[1, ].toarray()[0])[::-1]\n",
    "\n",
    "#nonzero terms\n",
    "terms_sorted_tfidf_left = [features[i] for i in idx_sorted_tfidf_left if dtm_side[0,i]] \n",
    "terms_sorted_tfidf_right = [features[i] for i in idx_sorted_tfidf_right if dtm_side[1,i]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's put together this information for plotting\n",
    "pd.DataFrame({'term': terms_sorted_tfidf_left[:1000],\n",
    "              'tfidf': dtm_side_tfidf[0, idx_sorted_tfidf_left[:1000]].A[0],\n",
    "              'side': 'left'})\\\n",
    "  .to_csv('./output/ngrams_top1000_left.csv', index_label='rank')\n",
    "    \n",
    "pd.DataFrame({'term': terms_sorted_tfidf_right[:1000],\n",
    "              'tfidf': dtm_side_tfidf[1, idx_sorted_tfidf_right[:1000]].A[0],\n",
    "              'side': 'right'})\\\n",
    "  .to_csv('./output/ngrams_top1000_right.csv', index_label='rank')\n",
    "\n",
    "pd.DataFrame(dtm_source.T.toarray(),\n",
    "             columns=sources.base_url)\\\n",
    "  .assign(term=features)\\\n",
    "  .to_csv('./output/dtm_source_counts.csv', index=False)\n",
    "\n",
    "sources[['base_url', 'fb_id', 'side', 'avg_align']].to_csv('./output/source_info.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "qry = \"SELECT article_text, url, base_url FROM articles where lower(article_text) like '%%copyright 20__ the associated press%%'\"\n",
    "df = pd.read_sql(qry, create_engine('postgres://postgres:**password**@localhost/articles'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'advertisement_story_continues',\n",
       " u'donald_trump',\n",
       " u'hillary_clinton',\n",
       " u'united_states',\n",
       " u'planned_parenthood',\n",
       " u'white_house',\n",
       " u'president_obama',\n",
       " u'obama_administration',\n",
       " u'state_department',\n",
       " u'new_york',\n",
       " u'last_week',\n",
       " u'last_year',\n",
       " u'fox_news',\n",
       " u'ted_cruz',\n",
       " u'islamic_state',\n",
       " u'social_media',\n",
       " u'new_york_times',\n",
       " u'supreme_court',\n",
       " u'bill_clinton',\n",
       " u'republican_party',\n",
       " u'middle_east',\n",
       " u'barack_obama',\n",
       " u'even_though',\n",
       " u'federal_government',\n",
       " u'bernie_sanders',\n",
       " u'clinton_foundation',\n",
       " u'new_hampshire',\n",
       " u'gon_na',\n",
       " u'law_enforcement',\n",
       " u'dont_know',\n",
       " u'police_officers',\n",
       " u'holy_spirit',\n",
       " u'make_sure',\n",
       " u'last_month',\n",
       " u'president_barack_obama',\n",
       " u'dont_want',\n",
       " u'washington_post',\n",
       " u'black_lives_matter',\n",
       " u'washington_dc',\n",
       " u'former_secretary',\n",
       " u'south_carolina',\n",
       " u'mr_trump',\n",
       " u'gun_control',\n",
       " u'national_security',\n",
       " u'democratic_party',\n",
       " u'email_address',\n",
       " u'north_carolina',\n",
       " u'last_night',\n",
       " u'illegal_immigrants',\n",
       " u'jeb_bush',\n",
       " u'christian_leaders',\n",
       " u'hillary_clintons',\n",
       " u'marco_rubio',\n",
       " u'email_protected',\n",
       " u'new_york_city',\n",
       " u'nt_know',\n",
       " u'climate_change',\n",
       " u'foreign_policy',\n",
       " u'mainstream_media',\n",
       " u'looks_like',\n",
       " u'every_day',\n",
       " u'former_president',\n",
       " u'second_amendment',\n",
       " u'press_conference',\n",
       " u'general_election',\n",
       " u'ca_nt',\n",
       " u'facebook_page',\n",
       " u'high_school',\n",
       " u'police_officer',\n",
       " u'john_kasich',\n",
       " u'donald_trumps',\n",
       " u'justice_department',\n",
       " u'washington_examiner',\n",
       " u'two_years',\n",
       " u'ben_carson',\n",
       " u'mrs_clinton',\n",
       " u'saudi_arabia',\n",
       " u'health_care',\n",
       " u'donald_j_trump_realdonaldtrump',\n",
       " u'new_jersey',\n",
       " u'please_share',\n",
       " u'free_speech',\n",
       " u'vice_president',\n",
       " u'wall_street',\n",
       " u'tea_party',\n",
       " u'political_correctness',\n",
       " u'illegal_immigration',\n",
       " u'breitbart_news',\n",
       " u'took_place',\n",
       " u'syrian_refugees',\n",
       " u'republican_presidential_candidate',\n",
       " u'classified_information',\n",
       " u'pope_francis',\n",
       " u'mitt_romney',\n",
       " u'global_warming',\n",
       " u'daily_caller',\n",
       " u'presidential_campaign',\n",
       " u'nt_want',\n",
       " u'los_angeles',\n",
       " u'president_obamas']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms_sorted_tfidf_right[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'advertisement_story_continues',\n",
       " u'email_address',\n",
       " u'illegal_immigrants',\n",
       " u'email_protected',\n",
       " u'washington_examiner',\n",
       " u'mrs_clinton',\n",
       " u'please_share',\n",
       " u'illegal_immigration',\n",
       " u'breitbart_news',\n",
       " u'classified_information',\n",
       " u'daily_caller',\n",
       " u'body_parts',\n",
       " u'private_email_server',\n",
       " u'national_review',\n",
       " u'radical_islam',\n",
       " u'terrorist_attack',\n",
       " u'illegal_aliens',\n",
       " u'planned_parenthoods',\n",
       " u'daily_mail',\n",
       " u'muslim_brotherhood',\n",
       " u'judicial_watch',\n",
       " u'daily_signal',\n",
       " u'click_like',\n",
       " u'democrat_party',\n",
       " u'san_bernardino',\n",
       " u'politically_correct',\n",
       " u'iran_deal',\n",
       " u'aborted_babies',\n",
       " u'private_server',\n",
       " u'nuclear_deal',\n",
       " u'fbi_director_james_comey',\n",
       " u'gateway_pundit',\n",
       " u'sharia_law',\n",
       " u'jesus_christ',\n",
       " u'state_hillary_clinton',\n",
       " u'american_flag',\n",
       " u'obama_administrations',\n",
       " u'sen_ted_cruz',\n",
       " u'al_qaeda',\n",
       " u'liberal_media',\n",
       " u'secret_service',\n",
       " u'islamic_state_group',\n",
       " u'washington_free_beacon',\n",
       " u'human_life',\n",
       " u'unborn_babies',\n",
       " u'medical_progress',\n",
       " u'inspector_general',\n",
       " u'terror_attacks',\n",
       " u'catholic_church',\n",
       " u'new_york_post',\n",
       " u'unborn_child',\n",
       " u'little_girl',\n",
       " u'president_barack_obamas',\n",
       " u'told_fox_news',\n",
       " u'terror_attack',\n",
       " u'abortion_clinics',\n",
       " u'american_citizens',\n",
       " u'latest_video_at_videofoxnewscom',\n",
       " u'share_your_thoughts',\n",
       " u'state_john_kerry',\n",
       " u'unborn_children',\n",
       " u'comments_section',\n",
       " u'republican_primary',\n",
       " u'abortion_industry',\n",
       " u'gun_owners',\n",
       " u'air_force',\n",
       " u'bill_clintons',\n",
       " u'illegal_alien',\n",
       " u'got_ta',\n",
       " u'federal_reserve',\n",
       " u'republican_establishment',\n",
       " u'islamic_terrorism',\n",
       " u'joe_biden',\n",
       " u'abortion_clinic',\n",
       " u'cbs_news',\n",
       " u'gop_establishment',\n",
       " u'huma_abedin',\n",
       " u'open_borders',\n",
       " u'lives_matter',\n",
       " u'stock_market',\n",
       " u'twitter_account',\n",
       " u'bear_arms',\n",
       " u'texas_senator',\n",
       " u'republican_presidential_nominee_donald',\n",
       " u'illegal_immigrant',\n",
       " u'washington_times',\n",
       " u'fetal_tissue',\n",
       " u'terrorist_group',\n",
       " u'house_speaker_paul_ryan',\n",
       " u'dr_ben_carson',\n",
       " u'barack_obamas',\n",
       " u'donald_j_trump',\n",
       " u'democratic_presidential_nominee',\n",
       " u'election_cycle',\n",
       " u'rush_limbaugh',\n",
       " u'federal_funding',\n",
       " u'email_scandal',\n",
       " u'attorney_general_loretta_lynch',\n",
       " u'poll_numbers',\n",
       " u'ohio_gov',\n",
       " u'gop_nomination',\n",
       " u'free_beacon',\n",
       " u'recent_weeks',\n",
       " u'common_core',\n",
       " u'american_dream',\n",
       " u'told_wnd',\n",
       " u'texas_sen_ted_cruz',\n",
       " u'super_tuesday',\n",
       " u'reprinted_with_permission',\n",
       " u'gun_rights',\n",
       " u'southern_border',\n",
       " u'tim_kaine',\n",
       " u'sanctuary_cities',\n",
       " u'glenn_beck',\n",
       " u'omar_mateen',\n",
       " u'john_kerry',\n",
       " u'least_two',\n",
       " u'paris_attacks',\n",
       " u'assisted_suicide',\n",
       " u'donald_trump_said',\n",
       " u'sunday_morning',\n",
       " u'constitutional_right',\n",
       " u'contested_convention',\n",
       " u'white_privilege',\n",
       " u'roe_v_wade',\n",
       " u'asylum_seekers',\n",
       " u'conservative_movement',\n",
       " u'sen_marco_rubio',\n",
       " u'breitbart_texas',\n",
       " u'defund_planned_parenthood',\n",
       " u'tuesday_morning',\n",
       " u'hate_speech',\n",
       " u'terrorist_organization',\n",
       " u'tax_dollars',\n",
       " u'cell_phone',\n",
       " u'soviet_union',\n",
       " u'gop_candidate',\n",
       " u'law_enforcement_officers',\n",
       " u'pro-life_movement',\n",
       " u'muslim_migrants',\n",
       " u'rick_perry',\n",
       " u'terrorist_groups',\n",
       " u'border_security',\n",
       " u'marine_corps',\n",
       " u'friday_morning',\n",
       " u'phone_call',\n",
       " u'vice_president_joe_biden',\n",
       " u'past_week',\n",
       " u'christian_faith',\n",
       " u'monday_morning',\n",
       " u'first_ballot',\n",
       " u'college_campuses',\n",
       " u'due_process',\n",
       " u'fbi_investigation',\n",
       " u'operation_rescue',\n",
       " u'taxpayer_dollars',\n",
       " u'video_shows',\n",
       " u'ever_seen',\n",
       " u'per_cent',\n",
       " u'wasserman_schultz',\n",
       " u'weekly_standard',\n",
       " u'gop_primary',\n",
       " u'campaign_manager',\n",
       " u'armed_forces',\n",
       " u'nt_care',\n",
       " u'opened_fire',\n",
       " u'real_estate_mogul',\n",
       " u'doesnt_seem',\n",
       " u'sean_hannity',\n",
       " u'support_donald_trump']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[t for t in terms_sorted_tfidf_right[:500] if t not in terms_sorted_tfidf_left[:500]]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
